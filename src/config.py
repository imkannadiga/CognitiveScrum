import os

# Config for Llama 3 via Ollama
llm_config = {
    "model": "ollama/llama3", 
    "base_url": "http://localhost:11434"
}